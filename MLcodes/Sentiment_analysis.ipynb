{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T12:35:11.741700Z",
     "start_time": "2026-02-08T12:35:11.730042Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import string"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:35:14.331103Z",
     "start_time": "2026-02-08T12:35:14.283045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Pakistan\\Downloads\\all_tweets.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'text']\n",
    "\n",
    "print(f\"Shape:{df.shape}\")\n",
    "print(f\"\\nAll Sentiment values:\")\n",
    "print(df['sentiment'].value_counts().sort_index())\n",
    "print(df.head(10))"
   ],
   "id": "89f25015b053d121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:(6033, 5)\n",
      "\n",
      "All Sentiment values:\n",
      "sentiment\n",
      "0.0     65\n",
      "1.0     65\n",
      "2.0     65\n",
      "3.0     65\n",
      "4.0     65\n",
      "        ..\n",
      "95.0    32\n",
      "96.0    23\n",
      "97.0    15\n",
      "98.0    12\n",
      "99.0     5\n",
      "Name: count, Length: 100, dtype: int64\n",
      "   sentiment         id                                               date  \\\n",
      "0        NaN  sentiment                                               text   \n",
      "1        0.0   positive  RT @SchudioTv: Want to know more about #autism...   \n",
      "2        1.0   negative  We blame ourselves and feel worse. Start with ...   \n",
      "3        2.0   positive  RT @PsychiatristCNS: 130,000 patient years and...   \n",
      "4        3.0    neutral  RT @SkypeTherapist: See a therapist online ove...   \n",
      "5        4.0   positive  RT @PsychiatristCNS: 130,000 patient years and...   \n",
      "6        5.0    neutral  The onset of the #pandemic &amp; #WFH has led ...   \n",
      "7        6.0    neutral  #Climate change is concerning. \\n\\nThese can c...   \n",
      "8        7.0   positive  130,000 patient years and the diagnostic stabi...   \n",
      "9        8.0    neutral  Looking for online counseling via Skype?Â Skyp...   \n",
      "\n",
      "             query     text  \n",
      "0             user    label  \n",
      "1  beyondbehaviour  anxiety  \n",
      "2         cherie7c  anxiety  \n",
      "3        ThinkNeha  anxiety  \n",
      "4    FrankCoulson7  anxiety  \n",
      "5        Phcourtet  anxiety  \n",
      "6    ajhospitalmng  anxiety  \n",
      "7  585Mentalhealth  anxiety  \n",
      "8  PsychiatristCNS  anxiety  \n",
      "9   SkypeTherapist  anxiety  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:35:22.239236Z",
     "start_time": "2026-02-08T12:35:22.231990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text =text.lower()\n",
    "    text = re.sub(r'https\\S+|https\\S+\\www\\.\\S+', '', text)\n",
    "    text = re.sub(r'^@\\w+|#\\w+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = ''.join(text.split())\n",
    "    return text"
   ],
   "id": "a8411346d9721561",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T13:09:11.435184Z",
     "start_time": "2026-02-08T13:09:11.109504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "df['word_count'] = df['cleaned_text'].str.split().str.len()\n",
    "df = df[df['word_count'] >= 3]\n",
    "\n",
    "df = df[df['cleaned_text'].str.strip() != \"\"]\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts().sort_index()}\")\n",
    "\n",
    "min_sentiment = df['sentiment'].min()\n",
    "max_sentiment = df['sentiment'].max()\n",
    "\n",
    "print(f\"\\nMin sentiment: {min_sentiment}\")\n",
    "print(f\"Max sentiment: {max_sentiment}\")\n",
    "df['sentiment'] = df['sentiment'].map({min_sentiment: 0, max_sentiment: 1})\n",
    "df['sentiment'] = df['sentiment'].map({min_sentiment: 0, max_sentiment: 1})\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(f\"Negative: {(df['sentiment'] == 0).sum()}\")\n",
    "print(f\"Positive: {(df['sentiment'] == 1).sum()}\")\n",
    "\n",
    "\n",
    "print(\"\\n5 Negative examples:\")\n",
    "for text in df[df['sentiment'] == 0]['cleaned_text'].head(5):\n",
    "    print(f\"  - {text}\")\n",
    "\n",
    "print(\"\\n5 Positive examples:\")\n",
    "for text in df[df['sentiment'] == 1]['cleaned_text'].head(5):\n",
    "    print(f\"  - {text}\")\n",
    "\n",
    "\n",
    "if len(df) < 100:\n",
    "    print(\"\\nWARNING: Dataset too small! Need at least 50 samples.\")\n",
    "    print(\"Please check your CSV file or use a larger dataset.\")\n",
    "    print(\"\\nData looks good! Proceeding with training...\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['cleaned_text'],\n",
    "        df['sentiment'],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df['sentiment']\n",
    "    )\n",
    "\n",
    "print(f'\\nTraining size: {len(X_train)}')\n",
    "print(f'Test size: {len(X_test)}')"
   ],
   "id": "310e9f89a3532d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (0, 7)\n",
      "Sentiment distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Min sentiment: nan\n",
      "Max sentiment: nan\n",
      "(0, 7)\n",
      "Negative: 0\n",
      "Positive: 0\n",
      "\n",
      "5 Negative examples:\n",
      "\n",
      "5 Positive examples:\n",
      "\n",
      "WARNING: Dataset too small! Need at least 50 samples.\n",
      "Please check your CSV file or use a larger dataset.\n",
      "\n",
      "Data looks good! Proceeding with training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     36\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPlease check your CSV file or use a larger dataset.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     37\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mData looks good! Proceeding with training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m X_train, X_test, y_train, y_test = \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcleaned_text\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msentiment\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstratify\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msentiment\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(X_train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m     49\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mTest size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(X_test)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Machine-learning-journey\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Machine-learning-journey\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2924\u001B[39m, in \u001B[36mtrain_test_split\u001B[39m\u001B[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[39m\n\u001B[32m   2921\u001B[39m arrays = indexable(*arrays)\n\u001B[32m   2923\u001B[39m n_samples = _num_samples(arrays[\u001B[32m0\u001B[39m])\n\u001B[32m-> \u001B[39m\u001B[32m2924\u001B[39m n_train, n_test = \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2925\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.25\u001B[39;49m\n\u001B[32m   2926\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m   2929\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Machine-learning-journey\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2504\u001B[39m, in \u001B[36m_validate_shuffle_split\u001B[39m\u001B[34m(n_samples, test_size, train_size, default_test_size)\u001B[39m\n\u001B[32m   2501\u001B[39m n_train, n_test = \u001B[38;5;28mint\u001B[39m(n_train), \u001B[38;5;28mint\u001B[39m(n_test)\n\u001B[32m   2503\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_train == \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2504\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   2505\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWith n_samples=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m, test_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m and train_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m, the \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2506\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mresulting train set will be empty. Adjust any of the \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2507\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33maforementioned parameters.\u001B[39m\u001B[33m\"\u001B[39m.format(n_samples, test_size, train_size)\n\u001B[32m   2508\u001B[39m     )\n\u001B[32m   2510\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m n_train, n_test\n",
      "\u001B[31mValueError\u001B[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:27:47.859524Z",
     "start_time": "2026-02-08T12:27:47.829547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = Vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = Vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (test):\", X_test_tfidf.shape)"
   ],
   "id": "1e6438beb8281412",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (208, 4)\n",
      "TF-IDF shape (test): (52, 4)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:27:48.016462Z",
     "start_time": "2026-02-08T12:27:47.901924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "bnb_pred = bnb.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(pd.Series(bnb_pred).value_counts())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, bnb_pred))\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, bnb_pred, labels=[0, 1]))\n",
    "\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, bnb_pred, target_names=['negative', 'positive'], zero_division=0, labels=[0, 1]))"
   ],
   "id": "b55e493f376c396f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction distribution:\n",
      "1    45\n",
      "0     7\n",
      "Name: count, dtype: int64\n",
      "Accuracy: 0.4423076923076923\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2 24]\n",
      " [ 5 21]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.08      0.12        26\n",
      "    positive       0.47      0.81      0.59        26\n",
      "\n",
      "    accuracy                           0.44        52\n",
      "   macro avg       0.38      0.44      0.36        52\n",
      "weighted avg       0.38      0.44      0.36        52\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:27:56.742690Z",
     "start_time": "2026-02-08T12:27:56.514909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, lr_pred):.4f}\")\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    lr_pred,\n",
    "    target_names=['Negative', 'Positive']\n",
    "))\n",
    "\n",
    "\n",
    "svm = LinearSVC(max_iter=1000, random_state=42)\n",
    "\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, svm_pred):.4f}\")\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    svm_pred,\n",
    "    labels=[0, 1],\n",
    "    target_names=['Negative', 'Positive']\n",
    "))\n",
    "\n",
    "\n",
    "results = {\n",
    "    'Bernoulli Naive Bayes': accuracy_score(y_test, bnb_pred),\n",
    "    'Logistic Regression': accuracy_score(y_test, lr_pred),\n",
    "    'Linear SVM': accuracy_score(y_test, svm_pred)\n",
    "}\n",
    "\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {acc:.4f}\")\n"
   ],
   "id": "4768610ab53b5f0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.4423\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.29      0.08      0.12        26\n",
      "    Positive       0.47      0.81      0.59        26\n",
      "\n",
      "    accuracy                           0.44        52\n",
      "   macro avg       0.38      0.44      0.36        52\n",
      "weighted avg       0.38      0.44      0.36        52\n",
      "\n",
      "\n",
      "Accuracy: 0.4423\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.29      0.08      0.12        26\n",
      "    Positive       0.47      0.81      0.59        26\n",
      "\n",
      "    accuracy                           0.44        52\n",
      "   macro avg       0.38      0.44      0.36        52\n",
      "weighted avg       0.38      0.44      0.36        52\n",
      "\n",
      "Bernoulli Naive Bayes: 0.4423\n",
      "Logistic Regression: 0.4423\n",
      "Linear SVM: 0.4423\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3515f16f6f070e8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
